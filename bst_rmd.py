# -*- coding: utf-8 -*-
"""rmd_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v5ZSURp_JZFok38dbp7hH6WxnpHE-p-h
"""

import os
os.chdir('/content/drive/MyDrive/recommend/rmd')
import pandas as pd
import os
import math
from zipfile import ZipFile
from urllib.request import urlretrieve
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import StringLookup
import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="BST model for recommendation system")
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=0.01,
    )
    parser.add_argument(
        "--dropout_rate",
        type=float,
        default=0.1,
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=256,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument(
        "--shuffle",
        type=bool,
        default=True,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=10,
    )
    # dir
    parser.add_argument(
        "--path",
        type=str,
        default='../hahow/data/',
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default='bst_course_seen.csv',
    )
    parser.add_argument(
        "--debug",
        type=bool,
        default=False,
    )
    args, unknown = parser.parse_known_args()
    return args
args = parse_args()

def preprocess_rating(rating_data, user_data, course_data,
                     rating_col, user_col, course_col):
  # if user by this class, we assume that he rated this course as 1
  rating_data[rating_col] = 1.0
  rating_data[rating_col] = rating_data[rating_col].astype(float)
  # train_data = train_data.sort_values(by=[user_col])

  # 預設每一個使用者至少都看過預設課程（自創的，編號101，內容是hahow線上課程）
  defult_train = user_data[[user_col]]
  defult_train[rating_col] = int(1)
  defult_train[course_col] = '800'

  # appendd two data set, defualt data in first
  rating_data = defult_train.append(rating_data)
  # add time seris, assume original order in raw data is time seris
  rating_data['unix_timestamp'] = np.arange(len(rating_data))
  # split by space and transfer to user-movie-rating
  rating_data[course_col] = rating_data[course_col].apply(lambda x: x.split(' '))
  rating_data = rating_data.explode(course_col)

  # merge train data and get short id
  rating_data = rating_data.merge(user_data[['user_id', 'user_id_int']],
                                  left_on='user_id', right_on='user_id', how='left')
  rating_data = rating_data.merge(course_data[['course_id', 'course_id_int']],
                                  left_on='course_id', right_on='course_id', how='left')
  # remove unecessary columns
  del rating_data['course_id'], rating_data['user_id']
  rating_data = rating_data.rename(columns={'course_id_int': 'movie_id',
                                           'user_id_int': 'user_id'})
  # make sure raing is int type
  rating_data[rating_col] = rating_data[rating_col].astype(int)
  return rating_data

def longID2int(df, target_cols, new_cols):
  for col, new in zip(target_cols, new_cols):
    df[new] = df[col].astype('category').cat.codes.astype(int)
  return df

def preprocess_course(course_data):
  # add default class
  # course data preprocess
  # set a defult class that every user will take at first
  defult = pd.DataFrame({
      'course_id': ['800'],
      'course_name': ['線上課程'],
      'course_price': [0], 
      'teacher_id': ['101teacher'],
      'teacher_intro': ['線上課程'],
      'groups': ['教育'],
      'sub_groups': ['教育'],
  })
  # 在課程列表裡加入我們自創的在課程列表裡加入我們自創的 defult course 
  course_data = course_data.append(defult)
  # 把整個資料集裡面的把整個資料集裡面的 missing X 填補
  course_data = course_data.fillna('X')
  # 'genres' 欄位，把把subgroup用|合併
  course_data['genres'] = course_data['sub_groups'].fillna('其他').str.replace(',', '|')
  # convert long string id into short int type id
  course_data = longID2int(course_data, ['course_id'], ['course_id_int'])
  course_data = course_data.rename(columns={'course_name': 'title',
                                            }
                                   )
  return course_data

def preprocess_user(user_data):
  # convert long str in into int id
  user_data = longID2int(user_data, ['user_id'], ['user_id_int'])
  user_data = user_data.fillna('X')
  user_data['interests'] = user_data['interests'].str.replace(',', '_')
  # rename to fit sample code columns name
  user_data = user_data.rename(columns={'gender': 'sex',
                                        'interests': 'age_group',
                                        'occupation_titles': 'occupation',
                                        })
  return user_data

sequence_length = 2
step_size = 1

def create_sequences(values, window_size, step_size):
    sequences = []
    start_index = 0
    while True:
        end_index = start_index + window_size
        seq = values[start_index:end_index]
        if len(seq) < window_size:
            seq = values[-window_size:]
            if len(seq) == window_size:
                sequences.append(seq)
            break
        sequences.append(seq)
        start_index += step_size
    return sequences

def rating2input(ratings, split):
  ratings_group = ratings.sort_values(by=["unix_timestamp"]).groupby("user_id")

  ratings_data = pd.DataFrame(
      data={
          "user_id": list(ratings_group.groups.keys()),
          "movie_ids": list(ratings_group.movie_id.apply(list)),
          "ratings": list(ratings_group.rating.apply(list)),
          "timestamps": list(ratings_group.unix_timestamp.apply(list)),
      }
  )
  ratings_data.movie_ids = ratings_data.movie_ids.apply(
      lambda ids: create_sequences(ids, sequence_length, step_size)
  )

  ratings_data.ratings = ratings_data.ratings.apply(
      lambda ids: create_sequences(ids, sequence_length, step_size)
  )

  del ratings_data["timestamps"]
  ratings_data_movies = ratings_data[["user_id", "movie_ids"]].explode(
    "movie_ids", ignore_index=True
    )
  ratings_data_movies = ratings_data_movies.dropna(axis=0)
  ratings_data_rating = ratings_data[["ratings"]].explode("ratings", ignore_index=True)
  ratings_data_rating = ratings_data_rating.dropna(axis=0)
  ratings_data_transformed = pd.concat([ratings_data_movies, ratings_data_rating], axis=1)
  ratings_data_transformed = ratings_data_transformed.join(
      users.set_index("user_id"), on="user_id"
  )
  ratings_data_transformed.ratings = ratings_data_transformed.ratings.apply(
      lambda x: ",".join([str(float(v)) for v in x])
  )
  # del ratings_data_transformed["zip_code"]
  ratings_data_transformed.rename(
      columns={"movie_ids": "sequence_movie_ids", "ratings": "sequence_ratings"},
      inplace=True,
  )
  ratings_data_transformed['sequence_movie_ids'] = ratings_data_transformed['sequence_movie_ids'].apply(lambda x: str(x)[1:-1])
  ratings_data_transformed['sequence_movie_ids'] = ratings_data_transformed['sequence_movie_ids'].str.replace(' ', '')

  # drop_duplicates
  ratings_data_transformed = ratings_data_transformed.drop_duplicates()
  return ratings_data_transformed

def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):
    def process(features):
        movie_ids_string = features["sequence_movie_ids"]
        sequence_movie_ids = tf.strings.split(movie_ids_string, ",").to_tensor()

        # The last movie id in the sequence is the target movie.
        features["target_movie_id"] = sequence_movie_ids[:, -1]
        features["sequence_movie_ids"] = sequence_movie_ids[:, :-1]

        ratings_string = features["sequence_ratings"]
        sequence_ratings = tf.strings.to_number(
            tf.strings.split(ratings_string, ","), tf.dtypes.float32
        ).to_tensor()

        # The last rating in the sequence is the target for the model to predict.
        target = sequence_ratings[:, -1]
        features["sequence_ratings"] = sequence_ratings[:, :-1]
        print(target)
        print(features)
        return features, target

    dataset = tf.data.experimental.make_csv_dataset(
        csv_file_path,
        batch_size=batch_size,
        column_names=CSV_HEADER,
        num_epochs=1,
        header=False,
        field_delim="|",
        shuffle=shuffle,
    ).map(process)

    return dataset

def process(features):
        movie_ids_string = features["sequence_movie_ids"]
        sequence_movie_ids = tf.strings.split(movie_ids_string, ",").to_tensor()
        # The last movie id in the sequence is the target movie.
        features["target_movie_id"] = sequence_movie_ids[:, -1]
        features["sequence_movie_ids"] = sequence_movie_ids[:, :-1]
        ratings_string = features["sequence_ratings"]
        sequence_ratings = tf.strings.to_number(
            tf.strings.split(ratings_string, ","), tf.dtypes.float32
        ).to_tensor()
        # The last rating in the sequence is the target for the model to predict.
        target = sequence_ratings[:, -1]
        features["sequence_ratings"] = sequence_ratings[:, :-1]
        return features, target

"""## Create model inputs"""

def create_model_inputs():
    return {
        "user_id": layers.Input(name="user_id", shape=(1,), dtype=tf.string),
        "sequence_movie_ids": layers.Input(
            name="sequence_movie_ids", shape=(sequence_length - 1,), dtype=tf.string
        ),
        "target_movie_id": layers.Input(
            name="target_movie_id", shape=(1,), dtype=tf.string
        ),
        "sequence_ratings": layers.Input(
            name="sequence_ratings", shape=(sequence_length - 1,), dtype=tf.float32
        ),
        "sex": layers.Input(name="sex", shape=(1,), dtype=tf.string),
        "age_group": layers.Input(name="age_group", shape=(1,), dtype=tf.string),
        "occupation": layers.Input(name="occupation", shape=(1,), dtype=tf.string),
    }

"""## Encode input features

The `encode_input_features` method works as follows:

1. Each categorical user feature is encoded using `layers.Embedding`, with embedding
dimension equals to the square root of the vocabulary size of the feature.
The embeddings of these features are concatenated to form a single input tensor.

2. Each movie in the movie sequence and the target movie is encoded `layers.Embedding`,
where the dimension size is the square root of the number of movies.

3. A multi-hot genres vector for each movie is concatenated with its embedding vector,
and processed using a non-linear `layers.Dense` to output a vector of the same movie
embedding dimensions.

4. A positional embedding is added to each movie embedding in the sequence, and then
multiplied by its rating from the ratings sequence.

5. The target movie embedding is concatenated to the sequence movie embeddings, producing
a tensor with the shape of `[batch size, sequence length, embedding size]`, as expected
by the attention layer for the transformer architecture.

6. The method returns a tuple of two elements:  `encoded_transformer_features` and
`encoded_other_features`.
"""

def encode_input_features(
    inputs,
    include_user_id=True,
    include_user_features=True,
    include_movie_features=True,
):

    encoded_transformer_features = []
    encoded_other_features = []

    other_feature_names = []
    if include_user_id:
        other_feature_names.append("user_id")
    if include_user_features:
        other_feature_names.extend(USER_FEATURES)

    ## Encode user features
    for feature_name in other_feature_names:
        # Convert the string input values into integer indices.
        print('CATEGORICAL_FEATURES_WITH_VOCABULARY', CATEGORICAL_FEATURES_WITH_VOCABULARY)
        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]
        idx = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)(
            inputs[feature_name]
        )
        # Compute embedding dimensions
        embedding_dims = int(math.sqrt(len(vocabulary)))
        # Create an embedding layer with the specified dimensions.
        embedding_encoder = layers.Embedding(
            input_dim=len(vocabulary),
            output_dim=embedding_dims,
            name=f"{feature_name}_embedding",
        )
        # Convert the index values to embedding representations.
        encoded_other_features.append(embedding_encoder(idx))

    ## Create a single embedding vector for the user features
    if len(encoded_other_features) > 1:
        encoded_other_features = layers.concatenate(encoded_other_features)
    elif len(encoded_other_features) == 1:
        encoded_other_features = encoded_other_features[0]
    else:
        encoded_other_features = None

    ## Create a movie embedding encoder
    movie_vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY["movie_id"]
    movie_embedding_dims = int(math.sqrt(len(movie_vocabulary)))

    # movie vocab
    # print('movie_vocabulary', movie_vocabulary)
    movie_vocabulary = [str(vocab) for vocab in movie_vocabulary]
    
    # Create a lookup to convert string values to integer indices.
    movie_index_lookup = StringLookup(
        vocabulary=movie_vocabulary,
        mask_token=None,
        num_oov_indices=0,
        name="movie_index_lookup",
    )
    # Create an embedding layer with the specified dimensions.
    movie_embedding_encoder = layers.Embedding(
        input_dim=len(movie_vocabulary),
        output_dim=movie_embedding_dims,
        name=f"movie_embedding",
    )
    # Create a vector lookup for movie genres.
    genre_vectors = movie[genres].to_numpy()
    movie_genres_lookup = layers.Embedding(
        input_dim=genre_vectors.shape[0],
        output_dim=genre_vectors.shape[1],
        embeddings_initializer=tf.keras.initializers.Constant(genre_vectors),
        trainable=False,
        name="genres_vector",
    )
    # Create a processing layer for genres.
    movie_embedding_processor = layers.Dense(
        units=movie_embedding_dims,
        activation="relu",
        name="process_movie_embedding_with_genres",
    )

    ## Define a function to encode a given movie id.
    def encode_movie(movie_id):
        # Convert the string input values into integer indices.
        movie_idx = movie_index_lookup(movie_id)
        movie_embedding = movie_embedding_encoder(movie_idx)
        encoded_movie = movie_embedding
        if include_movie_features:
            movie_genres_vector = movie_genres_lookup(movie_idx)
            encoded_movie = movie_embedding_processor(
                layers.concatenate([movie_embedding, movie_genres_vector])
            )
        return encoded_movie

    ## Encoding target_movie_id
    target_movie_id = inputs["target_movie_id"]
    encoded_target_movie = encode_movie(target_movie_id)

    ## Encoding sequence movie_ids.
    sequence_movies_ids = inputs["sequence_movie_ids"]
    encoded_sequence_movies = encode_movie(sequence_movies_ids)
    # Create positional embedding.
    position_embedding_encoder = layers.Embedding(
        input_dim=sequence_length,
        output_dim=movie_embedding_dims,
        name="position_embedding",
    )
    positions = tf.range(start=0, limit=sequence_length - 1, delta=1)
    encodded_positions = position_embedding_encoder(positions)
    # Retrieve sequence ratings to incorporate them into the encoding of the movie.
    sequence_ratings = tf.expand_dims(inputs["sequence_ratings"], -1)
    # Add the positional encoding to the movie encodings and multiply them by rating.
    encoded_sequence_movies_with_poistion_and_rating = layers.Multiply()(
        [(encoded_sequence_movies + encodded_positions), sequence_ratings]
    )

    # Construct the transformer inputs.
    for encoded_movie in tf.unstack(
        encoded_sequence_movies_with_poistion_and_rating, axis=1
    ):
        encoded_transformer_features.append(tf.expand_dims(encoded_movie, 1))
    encoded_transformer_features.append(encoded_target_movie)

    encoded_transformer_features = layers.concatenate(
        encoded_transformer_features, axis=1
    )
    return encoded_transformer_features, encoded_other_features

"""## Create a BST model"""

include_user_id = False
include_user_features = False
include_movie_features = False

hidden_units = [256, 128]
dropout_rate = args.dropout_rate
num_heads = 3


def create_model():
    inputs = create_model_inputs()
    transformer_features, other_features = encode_input_features(
        inputs, include_user_id, include_user_features, include_movie_features
    )

    # Create a multi-headed attention layer.
    attention_output = layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=transformer_features.shape[2], dropout=dropout_rate
    )(transformer_features, transformer_features)

    # Transformer block.
    attention_output = layers.Dropout(dropout_rate)(attention_output)
    x1 = layers.Add()([transformer_features, attention_output])
    x1 = layers.LayerNormalization()(x1)
    x2 = layers.LeakyReLU()(x1)
    x2 = layers.Dense(units=x2.shape[-1])(x2)
    x2 = layers.Dropout(dropout_rate)(x2)
    transformer_features = layers.Add()([x1, x2])
    transformer_features = layers.LayerNormalization()(transformer_features)
    features = layers.Flatten()(transformer_features)

    # Included the other features.
    if other_features is not None:
        features = layers.concatenate(
            [features, layers.Reshape([other_features.shape[-1]])(other_features)]
        )

    # Fully-connected layers.
    for num_units in hidden_units:
        features = layers.Dense(num_units)(features)
        features = layers.BatchNormalization()(features)
        features = layers.LeakyReLU()(features)
        features = layers.Dropout(dropout_rate)(features)

    outputs = layers.Dense(units=1)(features)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

def preprocess_test(test_data, all_course_list):
  test_data['course_all'] = all_course_list
  def fun(x):
    x = x[1:-1]
    return x.split(',')
  test_data['course_all'] = test_data['course_all'].apply(lambda x: fun(x))
  test_data = test_data.explode('course_all')
  test_data['sequence_movie_ids'] = '711'+ ',' + test_data['course_all']
  test_data['sequence_movie_ids'] = test_data['sequence_movie_ids'].str.replace(' ', '')
  del test_data['course_all']
  return test_data

def long2wide(long_data):
  long_data = long_data.sort_values(by=['user_id','course_socre'], ascending=False)
  long_data = long_data.groupby('user_id',
                                as_index=False).agg({'user_id':'first',
                                                     'course_id':lambda x : ' '.join(x.astype(str))})
  return long_data

RATING_SPLIT = ['train', 'val_seen', 'test_seen']

"""# main"""

if __name__ == '__main__':

  # load data
  path = args.path
  data_list = os.listdir(path)
  data_dict = {}
  for data in data_list:
    name = data.split(".")[0]
    data_dict[name] = pd.read_csv(path+data)
  
  # preprocess
  movie = preprocess_course(data_dict['courses'])
  users = preprocess_user(data_dict['users'])
  rating_dict = dict()
  for split in RATING_SPLIT:
    rating_dict[split] = preprocess_rating(data_dict[split], users, movie,
                                   'rating', 'user_id', 'course_id')

  # # remove unecessary col
  del users['user_id']
  users = users.rename(columns={'user_id_int': 'user_id'})

  # genres
  genres = data_dict['subgroups']['subgroup_name'].tolist()
  for genre in genres:
      movie[genre] = movie["genres"].apply(
          lambda values: int(genre in values.split("|"))
      )

  # rating to input
  for split in RATING_SPLIT:
    rating_dict[split] = rating2input(rating_dict[split], split)
    if split == 'test_seen':
      all_course = str(movie.course_id_int.tolist())
      rating_dict[split] = preprocess_test(rating_dict[split], all_course)
    rating_dict[split].to_csv(f"{split}_input.csv", index=False, sep="|", header=False)

  # define metadata
  CSV_HEADER = list(rating_dict['train'].columns)
  CATEGORICAL_FEATURES_WITH_VOCABULARY = {
      "user_id": list(users.user_id.unique()),
      "movie_id": list(movie.course_id_int.unique()),
      "sex": list(users.sex.unique()),
      "age_group": list(users.age_group.unique()),
      "occupation": list(users.occupation.unique()),
  }
  USER_FEATURES = ["sex", "age_group", "occupation"]
  MOVIE_FEATURES = ["genres"]

  dataset_dict = dict() # creat dataset
  t_dict = dict() # feature

  """ parameter """
  batch_size = args.batch_size
  shuffle = args.shuffle
  num_epochs = args.num_epochs
  learning_rate = args.learning_rate
  """ parameter """

  # creat dataset and feature
  for split in RATING_SPLIT:
    csv_file_path = f"{split}_input.csv"
    print(csv_file_path)
    dataset_dict[split] = get_dataset_from_csv(csv_file_path,
                                               shuffle=shuffle,
                                               batch_size=batch_size,
                                               )
  t_dict[split] = tf.data.experimental.make_csv_dataset(
        csv_file_path,
        batch_size=batch_size,
        column_names=CSV_HEADER,
        num_epochs=num_epochs,
        header=False,
        field_delim="|",
        shuffle=shuffle,
    ).map(process)
  model = create_model()
  # Compile the model.
  model.compile(
      optimizer=keras.optimizers.Adagrad(learning_rate=learning_rate),
      loss=keras.losses.MeanSquaredError(),
      metrics=[keras.metrics.MeanAbsoluteError()],
  )
  # Fit the model with the training data.
  if args.debug == False:
    history = model.fit(dataset_dict['train'], epochs=num_epochs, validation_data=dataset_dict['val_seen'])
  # Read the test data.
  test_dataset = get_dataset_from_csv("test_seen_input.csv", batch_size=265)
  pred_test = model.predict(test_dataset)
  # convert predict outcome to user level data
  users = preprocess_user(data_dict['users'])
  rating_dict['test_seen']['course_socre'] = [i[0] for i in pred_test.tolist()]
  rating_dict['test_seen']['course_id_int'] = rating_dict['test_seen']['sequence_movie_ids'].str.replace('711,', '').astype(int)
  rating_dict['test_seen'] = rating_dict['test_seen'].merge(movie[['course_id_int', 'course_id']], on='course_id_int', how='left')
  rating_dict['test_seen'] = rating_dict['test_seen'].rename(columns={'user_id':'user_id_int'})
  rating_dict['test_seen'] = rating_dict['test_seen'].merge(users[['user_id_int', 'user_id']], on='user_id_int', how='left')
  result_long = rating_dict['test_seen'][['user_id', 'course_id', 'course_socre', ]]
  result_long = result_long[result_long['course_id']!='800']
  result = long2wide(result_long)
  result.to_csv(args.output_dir, index=False)

'''plot'''
# import matplotlib.pyplot as plt
# # plot acc mae
# plt.plot(history.history['val_loss'], label='validation')
# plt.plot(history.history['loss'], label='train')
# plt.ylabel('loss')
# plt.xlabel('Epoch')
# plt.title(f'dropout = {dropout_rate}')
# plt.legend()
# plt.savefig(fname=f'{dropout_rate}.png', dpi=800)
# plt.show()